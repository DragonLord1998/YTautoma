{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "A100"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üé¨ YTautoma - YouTube Shorts Automation\n",
                "\n",
                "Generate 60-second YouTube Shorts using local AI models:\n",
                "- **Story**: Gemma 3 (via Ollama)\n",
                "- **Images**: Z-Image-Turbo\n",
                "- **Video**: Wan 2.2\n",
                "- **Voice**: VibeVoice\n",
                "\n",
                "**Works on**: Colab (A100), RunPod, Lambda Labs, etc."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone YTautoma\n",
                "import os\n",
                "\n",
                "# Auto-detect workspace\n",
                "if os.path.exists('/content'):\n",
                "    WORKSPACE = '/content'\n",
                "elif os.path.exists('/workspace'):\n",
                "    WORKSPACE = '/workspace'\n",
                "else:\n",
                "    WORKSPACE = os.path.expanduser('~')\n",
                "\n",
                "os.chdir(WORKSPACE)\n",
                "print(f'Workspace: {WORKSPACE}')\n",
                "\n",
                "!git clone https://github.com/DragonLord1998/YTautoma.git\n",
                "os.chdir('YTautoma')\n",
                "PROJECT_DIR = os.getcwd()\n",
                "print(f'Project: {PROJECT_DIR}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q -r requirements.txt\n",
                "!pip install -q git+https://github.com/huggingface/diffusers\n",
                "!pip install -q flash-attn --no-build-isolation 2>/dev/null || echo 'flash-attn optional'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install Ollama\n",
                "!curl -fsSL https://ollama.com/install.sh | sh\n",
                "\n",
                "# Start Ollama in background\n",
                "import subprocess\n",
                "import time\n",
                "subprocess.Popen(['ollama', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
                "time.sleep(5)\n",
                "\n",
                "# Pull Gemma 3 (use smaller model for cloud GPUs)\n",
                "!ollama pull gemma3:4b"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone Wan 2.2\n",
                "!mkdir -p models\n",
                "!git clone https://github.com/Wan-Video/Wan2.2.git models/Wan2.2\n",
                "!pip install -q -r models/Wan2.2/requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download Wan 2.2 TI2V-5B (smaller model, works on most GPUs)\n",
                "!pip install -q \"huggingface_hub[cli]\"\n",
                "!huggingface-cli download Wan-AI/Wan2.2-TI2V-5B --local-dir models/Wan2.2-TI2V-5B"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone VibeVoice\n",
                "!git clone https://github.com/microsoft/VibeVoice.git models/VibeVoice\n",
                "!pip install -q -e models/VibeVoice"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create .env configuration (auto-detect paths)\n",
                "import os\n",
                "PROJECT_DIR = os.getcwd()\n",
                "\n",
                "env_content = f\"\"\"OLLAMA_MODEL=gemma3:4b\n",
                "OLLAMA_BASE_URL=http://localhost:11434\n",
                "\n",
                "ZIMAGE_MODEL=Tongyi-MAI/Z-Image-Turbo\n",
                "ZIMAGE_DEVICE=cuda\n",
                "\n",
                "WAN_REPO_PATH={PROJECT_DIR}/models/Wan2.2\n",
                "WAN_MODEL_PATH={PROJECT_DIR}/models/Wan2.2-TI2V-5B\n",
                "WAN_T5_CPU=true\n",
                "WAN_OFFLOAD_MODEL=true\n",
                "\n",
                "VIBEVOICE_REPO_PATH={PROJECT_DIR}/models/VibeVoice\n",
                "VIBEVOICE_MODEL=microsoft/VibeVoice-Realtime-0.5B\n",
                "VIBEVOICE_SPEAKER=Carter\n",
                "\n",
                "LOW_VRAM_MODE=true\n",
                "TORCH_DTYPE=float16\n",
                "\"\"\"\n",
                "\n",
                "with open('.env', 'w') as f:\n",
                "    f.write(env_content)\n",
                "\n",
                "print('‚úÖ Configuration saved!')\n",
                "print(f'   Project: {PROJECT_DIR}')\n",
                "print(f'   VibeVoice: {PROJECT_DIR}/models/VibeVoice')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Generate YouTube Short"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate story only (quick test)\n",
                "!python main.py --story-only -c mystery"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate images only (no video)\n",
                "!python main.py --images-only -c horror"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Full pipeline\n",
                "!python main.py -c sci-fi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Download Output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List generated files\n",
                "!ls -la output/\n",
                "!find output -name '*.mp4' -o -name '*.png' | head -20"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download (Colab only)\n",
                "import os\n",
                "import glob\n",
                "\n",
                "try:\n",
                "    from google.colab import files\n",
                "    videos = glob.glob('output/**/*.mp4', recursive=True)\n",
                "    if videos:\n",
                "        latest = max(videos, key=lambda x: os.path.getmtime(x))\n",
                "        print(f'Downloading: {latest}')\n",
                "        files.download(latest)\n",
                "    else:\n",
                "        print('No video found. Run the pipeline first!')\n",
                "except ImportError:\n",
                "    print('Not in Colab. Find your video at:')\n",
                "    !find output -name '*.mp4'"
            ]
        }
    ]
}